{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65764868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "ENCODING DÉTECTÉ = ISO-8859-1\n",
      "\n",
      "RÉPONSE :\n",
      "Voici un résumé des conversations :\n",
      "\n",
      "Conversation 1:\n",
      "\n",
      "- La personne appelle un assistant pour valider ou attendre les résultats de l'I U T.\n",
      "- Elle explique qu'elle avait déjà validé, mais ne voyait pas le résultat sur l'écran.\n",
      "- L'assistant lui propose de regarder après.\n",
      "\n",
      "Conversation 2 (sujets abordés) :\n",
      "\n",
      "- La personne dit avoir un sujet à discuter demain matin.\n",
      "- Elle explique qu'elle a peu de temps et doit appeler rapidement.\n",
      "- L'assistant lui demande ce que le sujet est.\n",
      "- La personne explique qu'il n'est pas là, mais elle veut prévenir quelqu'un d'autre.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# RAG LOCAL COMPLET AVEC OLLAMA + POSTGRESQL + PGVECTOR\n",
    "# Version finale clean (projet académique)\n",
    "# ======================================================\n",
    "\n",
    "# ============================\n",
    "# IMPORTATIONS\n",
    "# ============================\n",
    "import psycopg\n",
    "from psycopg import Cursor\n",
    "import ollama\n",
    "import chardet\n",
    "\n",
    "# ============================\n",
    "# CONFIGURATION\n",
    "# ============================\n",
    "DB_CONNECTION_STR = \"dbname=rag_chatbot user=postgres password=summer2025 host=localhost port=5432\"\n",
    "CONVERSATION_FILES = [\n",
    "    \"../data/017_00000012.txt\",\n",
    "    \"../data/018_00000013.txt\",\n",
    "    \"../data/019_00000014.txt\",\n",
    "    \"../data/020_00000015.txt\",\n",
    "    \"../data/022_00000017.txt\"\n",
    "]\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"   # 768 dimensions\n",
    "LLM_MODEL = \"llama3.2\"                  # modèle génératif\n",
    "\n",
    "# ============================\n",
    "# 1. CHARGEMENT ET NETTOYAGE DU TEXTE\n",
    "# ============================\n",
    "\n",
    "def load_conversation(file_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Charge le fichier texte, détecte l'encodage\n",
    "    et retourne une liste de lignes nettoyées.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "        encoding = chardet.detect(raw)[\"encoding\"]\n",
    "        print(\"ENCODING DÉTECTÉ =\", encoding)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "\n",
    "    cleaned = [\n",
    "        line.strip().lstrip(\" \")\n",
    "        for line in lines\n",
    "        if line.strip() != \"\" and not line.startswith(\"<\")\n",
    "    ]\n",
    "    return cleaned\n",
    "\n",
    "# ============================\n",
    "# 2. REGROUPEMENT DU DIALOGUE\n",
    "# ============================\n",
    "\n",
    "def group_dialogue(lines: list[str], turns_per_chunk: int = 6) -> list[str]:\n",
    "    \"\"\"\n",
    "    Regroupe les tours de parole pour préserver\n",
    "    le sens de la conversation.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(lines), turns_per_chunk):\n",
    "        chunk = \" \".join(lines[i:i + turns_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# ============================\n",
    "# 3. CHUNKING CONTRÔLÉ (SAFE POUR EMBEDDINGS)\n",
    "# ============================\n",
    "\n",
    "def chunk_text(text: str, max_chars: int = 800) -> list[str]:\n",
    "    \"\"\"\n",
    "    Découpe un texte long en sous-chunks\n",
    "    (≈ 200 tokens) sans dépendre d'un tokenizer spécifique.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for sentence in text.split(\".\"):\n",
    "        if len(current) + len(sentence) < max_chars:\n",
    "            current += sentence + \".\"\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = sentence + \".\"\n",
    "\n",
    "    if current.strip():\n",
    "        chunks.append(current.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ============================\n",
    "# 4. EMBEDDINGS OLLAMA\n",
    "# ============================\n",
    "\n",
    "def calculate_embedding(text: str) -> list[float]:\n",
    "    response = ollama.embeddings(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        prompt=text\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "# ============================\n",
    "# 5. SAUVEGARDE EN BASE\n",
    "# ============================\n",
    "\n",
    "def save_embedding(corpus: str, embedding: list[float], cursor: Cursor) -> None:\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s)\",\n",
    "        (corpus, embedding)\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 6. RECHERCHE PAR SIMILARITÉ\n",
    "# ============================\n",
    "\n",
    "def similar_corpus(query: str, k: int = 3):\n",
    "    query_embedding = calculate_embedding(query)\n",
    "    vector_literal = \"[\" + \",\".join(map(str, query_embedding)) + \"]\"\n",
    "\n",
    "    with psycopg.connect(DB_CONNECTION_STR) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT corpus, embedding <=> %s::vector AS distance\n",
    "                FROM embeddings\n",
    "                ORDER BY distance ASC\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (vector_literal, k)\n",
    "            )\n",
    "            return cur.fetchall()\n",
    "\n",
    "# ============================\n",
    "# 7. RAG : GÉNÉRATION DE RÉPONSE\n",
    "# ============================\n",
    "\n",
    "def rag_answer(question: str) -> str:\n",
    "    results = similar_corpus(question)\n",
    "    context = \"\\n\\n\".join([r[0] for r in results])\n",
    "    prompt = f\"\"\"\n",
    "        Tu es un assistant d'analyse de documents.\n",
    "\n",
    "            CONTEXTE :{context}\n",
    "            QUESTION :{question}\n",
    "            INSTRUCTIONS :\n",
    "                - Réponds uniquement à partir du contexte\n",
    "                - Si plusieurs documents sont concernés, synthétise les informations\n",
    "                - N'ajoute aucune information externe\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# ============================\n",
    "# 8. INGESTION DES DONNÉES\n",
    "# ============================\n",
    "\n",
    "with psycopg.connect(DB_CONNECTION_STR) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "        cur.execute(\"DROP TABLE IF EXISTS embeddings\")\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE embeddings (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                corpus TEXT NOT NULL,\n",
    "                embedding VECTOR(768)\n",
    "            );\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        for file_path in CONVERSATION_FILES:\n",
    "            lines = load_conversation(file_path)\n",
    "            dialogue_chunks = group_dialogue(lines)\n",
    "\n",
    "            for dialogue in dialogue_chunks:\n",
    "                sub_chunks = chunk_text(dialogue)\n",
    "                for chunk in sub_chunks:\n",
    "                    emb = calculate_embedding(chunk)\n",
    "                    save_embedding(chunk, emb, cur)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 9. TEST FINAL\n",
    "# ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        question = input(\"\\nPose ta question (ou 'exit' pour quitter) : \")\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        print(\"\\nRÉPONSE :\")\n",
    "        print(rag_answer(question))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
